{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Immigration Analysis\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The main purpose of the project is to create the data model which will allows to analyze the immigration trends.\n",
    "For this purpose there needs to be implemented the ETL process which will be executed on a regular basis and will be responsible for cleaning, extracting and loading the data.\n",
    "The final data model can be used for verifying the correlation between:\n",
    " - destination temperature and immigration statistics\n",
    " - destination in the U.S and the source country\n",
    " - destination in the U.S and the source climates\n",
    " - arrival month and number of immigrants\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The end solution will make use of the Airflow workflow system which will call all the ETL stages on monthly basis.\n",
    "For processing (cleaning/transforming) the immigration data there will be used the Apache Spark.\n",
    "The Apache Spark output will be saved into the S3 buckets.\n",
    "Finally, the saved data will be loaded into the Redshift cluster for the business analytics queries.\n",
    "\n",
    "![ETL Architecture](https://lucid.app/publicSegments/view/9943301e-f97d-4cfa-b0df-0ee1a3ec45ab/image.png)\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The project will be based on the immigration dataset as a primary dataset and supplementary datasets like demographics, temperatures and aircodes.\n",
    "\n",
    " - **Immigration to the United States** ([source](https://travel.trade.gov/research/reports/i94/historical/2016.html))\n",
    " - U.S. city demographics ([source](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/))\n",
    " - Airport codes ([source](https://datahub.io/core/airport-codes#data))\n",
    " - Temperatures ([source](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data))\n",
    " \n",
    "This project will use the star schema with immigration as the fact table, whereas the other datasets serve as dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Data preparation\n",
    "To simplify and speed up the analysis part for the large dataset files there were created sample files with 10,000 records.\n",
    "Additionally, the data analysis is carried out in Pandas rather than in PySpark, but the final ETL process is implemented in PySpark.\n",
    "All the analyzed data are provided in the /data directory attached to this notebook (the sample files has _sample prefix):\n",
    "- immigration_sample.cvs - generated from file **i94_apr16_sub.sas7bdat** (sparkDF.sample(False, 0.01).limit(10000))\n",
    "- us-cities-demographics.csv - provided by the Udacity\n",
    "- airport-codes_csv.csv - provided by the Udacity\n",
    "- temperature_sample.csv - generated from file GlobalLandTemperaturesByCity.csv (pandasDF.sample(10000))\n",
    "\n",
    "To validate the immigration dataset there were created following dictionaries from udacity provided **I94_SAS_Labels_Descriptions.SAS** file:\n",
    "- country_codes.csv - list of valid country codes for birth and residence country column validation\n",
    "- us_ports.csv - list of valid us port codes for us port column validation\n",
    "- us_states.csv - list of valid us state codes for us state column validation\n",
    "- visa_modes.csv - list of visa modes identifiers for visa mode column validation\n",
    "\n",
    "and dictionaries from external sources:\n",
    "- visat_types.csv - list of visa types [source](https://www.trade.gov/visitor-arrivals-program-i-94-data).\n",
    "\n",
    "\n",
    "#### Explore the Data \n",
    "To understand the data there was done the exploratory data analysis (EDA). The results of this analysis allowed to judge the data quality and drive the conceptual data model creation.\n",
    "\n",
    "#### Cleaning Steps\n",
    "The cleaning process were dependent on the dataset, thus not all the steps were executed for each dataset.\n",
    "Generally, the process was divided into steps which were carried out in the following order:\n",
    "- **rename columns** (first steps in which columns are renamed to more meaningful names which are used later through the process)\n",
    "- **remove missing features/values** (the features (columns) which have more than 50% missing values are removed from the final dataset)\n",
    "- **validate data** (the columns values are validated against provided dictionaries)\n",
    "- **remove duplicates** (the rows which are duplicated are removed)\n",
    "- **cast data** (the column types are casted to more suitable data types (ex. after loading data some column types are assigned float type were can be represented by the integer type))\n",
    "- **convert data** (this step is applied to all columns where data values needs processing which cannot be achieved by casting (ex. converting int date type to datetime data type))\n",
    "\n",
    "The data processing order is important since the first steps (empty features, data validation, duplicated values) removes unnecessary data, and then the data manipulation steps works on smaller dataset.\n",
    "\n",
    "![Cleaning steps](https://app.lucidchart.com/publicSegments/view/6b8baf08-a6dc-4e08-a3d4-10b11f769068/image.png \"Cleaning steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration data\n",
    "\n",
    "This dataset comes from the US National Tourism and Trade Office. An I-**94** known as the Arrival-Departure Record Card is confirmation that a foreign national was allowed entry into the country. It keeps track of during any entries into the US in any nonimmigrant visa status.\n",
    "This dataset contains the below information.\n",
    "The 'source column' is the column name get from the original dataset and the 'RenameTo column' is the column which will be used in the rename step.\n",
    "\n",
    "| Source column | RenameTo column               | Description                                                                                      |\n",
    "|---------------|----------------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| CICID         |  Id                              | CICID is a unique number for the immigrants                                                      |\n",
    "| I94YR         |  year                            | 4 digit year                                                                                     |\n",
    "| I94MON        |  month                           | Numeric month                                                                                    |\n",
    "| I94CIT        |  birth_country                   | country of citizenship                                                                           |\n",
    "| I94RES        |  residence_country               | country of residence                                                                             |\n",
    "| I94PORT       |  port                            | 3 character code of US city destination                                                          |\n",
    "| ARRDATE       |  arrival_date                    | Arrival date in the US                                                                           |\n",
    "| I94MODE       |  arrival_type                    | 1 digit travel code (1 = 'Air', 2 = 'Sea', 3 = 'Land', 9 = 'Not reported')                       |\n",
    "| I94ADDR       |  us_state                        | Address where the immigrants resides in US state                                                 |\n",
    "| DEPDATE       |  departure_date                  | Departure Date from the US                                                                       |\n",
    "| I94BIR        |  person_age                      | Respondent age  in Years                                                                         |\n",
    "| I94VISA       |  visa_mode                       | Visa codes                                                                                       |\n",
    "| COUNT         |  count                           | Used for summary statistics                                                                      |\n",
    "| DTADFILE      |  added_date                      | Character Date Field - Date added to I-94 Files - CIC does not use                               |\n",
    "| VISAPOST      |  visa_issue_department           | Department of State where where Visa was issued - CIC does not use                               |\n",
    "| OCCUP         |  occupation                      | Occupation that will be performed in US  - CIC does not use                                      |\n",
    "| ENTDEPA       |  arrival_flag                    | Arrival Flag - admitted or paroled into the US - CIC does not use                                |\n",
    "| ENTDEPD       |  departure_flag                  | Departure Flag - Departed, lost I-94 or is deceased - CIC does not use                           |\n",
    "| ENTDEPU       |  update_flag                     | Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use      |\n",
    "| MATFLAG       |  match_arrival_departure_flag    | Match flag - Match of arrival and departure records                                              |\n",
    "| BIRYEAR       |  birth_year                      | 4 digit year of birth                                                                            |\n",
    "| DTADDTO       |  allowed_stay_date               | Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use |\n",
    "| GENDER        |  gender                          | Non-immigrant sex                                                                                |\n",
    "| INSNUM        |  ins_number                      | INS number                                                                                       |\n",
    "| AIRLINE       |  airline                         | Airline used to arrive in U.S                                                                    |\n",
    "| ADMNUM        |  admission_number                | Admission number                                                                                 |\n",
    "| FLTNO         |  flight_number                   | Flight number of airline used to arrive in US                                                    |\n",
    "| VISATYPE      |  visa_type                       | Type of visa which one owns                                                                      |\n",
    "\n",
    "##### Data source\n",
    "This is the main dataset with file for each month of 2016 year saved in [SAS file format](https://github.com/saurfang/spark-sas7bdat).</br>\n",
    "The average files size is about 500MB and contains 3 milion records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12,8)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the immigration data sample\n",
    "df_im = pd.read_csv('data/immigration_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head data\n",
    "df_im.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_im.rename(columns= {'cicid': 'Id',  \n",
    "          'i94yr': 'year' ,  \n",
    "          'i94mon': 'month' ,  \n",
    "          'i94cit': 'birth_country' ,  \n",
    "          'i94res': 'residence_country' ,  \n",
    "          'i94port': 'port' ,  \n",
    "          'arrdate': 'arrival_date' ,  \n",
    "          'i94mode': 'arrival_type' ,  \n",
    "          'i94addr': 'us_state' ,  \n",
    "          'depdate': 'departure_date' ,  \n",
    "          'i94bir': 'person_age' ,  \n",
    "          'i94visa': 'visa_mode' ,  \n",
    "          'count': 'ccount',\n",
    "          'dtadfile': 'added_date' ,  \n",
    "          'visapost': 'visa_issue_department' ,  \n",
    "          'occup': 'occupation' ,  \n",
    "          'entdepa': 'arrival_flag' ,  \n",
    "          'entdepd': 'departure_flag' ,  \n",
    "          'entdepu': 'update_flag' ,  \n",
    "          'matflag': 'match_arrival_departure_flag' ,  \n",
    "          'biryear': 'birth_year' ,  \n",
    "          'dtaddto': 'allowed_stay_date' ,  \n",
    "          'gender': 'gender' ,  \n",
    "          'insnum': 'ins_number' ,  \n",
    "          'airline': 'airline' ,  \n",
    "          'admnum': 'admission_number' ,  \n",
    "          'fltno': 'flight_number' ,  \n",
    "          'visatype': 'visa_type'},  \n",
    "         inplace=True)\n",
    "\n",
    "df_im.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the missing data using the heatmap\n",
    "# the concept was adopted from the article: https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d\n",
    "def showMissingDataHeatMap(df):\n",
    "    pct_dict = []\n",
    "    for col in df.columns:\n",
    "        pct_missing = np.mean(df[col].isnull())\n",
    "        pct = round(pct_missing*100)\n",
    "        if pct > 0:\n",
    "            pct_dict.append((col, pct))\n",
    "\n",
    "    pct_dict.sort(key=lambda x:x[1], reverse=True)\n",
    "    for key, val in pct_dict:\n",
    "        print('{} - {}%'.format(key, val))\n",
    "\n",
    "    cols_with_missing = [x[0] for x in pct_dict]\n",
    "    # heat map for columns with missing values sorted by missing pct vals\n",
    "    colours = ['#000099', '#ffff00'] # specify the colours - yellow is missing. blue is not missing.\n",
    "    sns.heatmap(df[cols_with_missing].isnull(), cmap=sns.color_palette(colours))\n",
    "\n",
    "showMissingDataHeatMap(df_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features which have more that 50% of missing values.\n",
    "# From the above we see that following features met above condition: occupation, update_flag, ins_number, visa_issue_department\n",
    "df_im.drop(columns=['occupation', 'update_flag', 'ins_number', 'visa_issue_department'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate against 'country code' dictionary\n",
    "df_dict_country_codes = pd.read_csv('data/dicts/country_code.csv')\n",
    "count_before = df_im.shape[0]\n",
    "res_country_filter = df_im['residence_country'].isin(df_dict_country_codes['code'])\n",
    "birt_country_filter = df_im['birth_country'].isin(df_dict_country_codes['code'])\n",
    "df_im = df_im[res_country_filter & birt_country_filter]         \n",
    "print('Removed rows after residence and birth country validation: {}%'.format(((count_before-df_im.shape[0])/10000)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate against 'us port' dictionary\n",
    "df_dict_port_codes = pd.read_csv('data/dicts/us_port.csv')\n",
    "count_before = df_im.shape[0]\n",
    "port_filter = df_im['port'].isin(df_dict_port_codes['code'])\n",
    "df_im = df_im[port_filter]         \n",
    "print('Removed rows after port validation: {}%'.format(((count_before-df_im.shape[0])/10000)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate against 'us states' dictionary\n",
    "df_dict_states_codes = pd.read_csv('data/dicts/us_state.csv')\n",
    "count_before = df_im.shape[0]\n",
    "state_filter = df_im['us_state'].isin(df_dict_states_codes['code'])\n",
    "df_im = df_im[state_filter]         \n",
    "print('Removed rows after us state validation: {}%'.format(((count_before-df_im.shape[0])/10000)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate against 'visa modes' dictionary\n",
    "df_dict_visa_modes = pd.read_csv('data/dicts/visa_mode.csv')\n",
    "count_before = df_im.shape[0]\n",
    "visa_mode_filter = df_im['visa_mode'].isin(df_dict_visa_modes['id'])\n",
    "df_im = df_im[visa_mode_filter]         \n",
    "print('Removed rows after visa mode validation: {}%'.format(((count_before-df_im.shape[0])/10000)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate against 'visa types' dictionary\n",
    "df_dict_visa_types = pd.read_csv('data/dicts/visa_type.csv')\n",
    "count_before = df_im.shape[0]\n",
    "\n",
    "# normalize data\n",
    "df_im['visa_type'] = df_im['visa_type'].str.strip().str.upper()\n",
    "\n",
    "visa_type_filter = df_im['visa_type'].isin(df_dict_visa_types['code'])\n",
    "df_im = df_im[visa_type_filter]    \n",
    "\n",
    "print('Removed rows after visa type validation: {}%'.format(((count_before-df_im.shape[0])/10000)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate against 'arrival types' dictionary\n",
    "df_dict_arrival_types = pd.read_csv('data/dicts/arrival_type.csv')\n",
    "count_before = df_im.shape[0]\n",
    "\n",
    "arrival_type_filter = df_im['arrival_type'].isin(df_dict_arrival_types['id'])\n",
    "df_im = df_im[arrival_type_filter]    \n",
    "\n",
    "print('Removed rows after arrival type validation: {}%'.format(((count_before-df_im.shape[0])/10000)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast immigration float data types to int\n",
    "im_column_names_to_int = ['Id', 'year', 'month', 'birth_country', 'residence_country',\n",
    "                          'arrival_date', 'arrival_type', 'departure_date', 'person_age', 'visa_mode',\n",
    "                          'ccount', 'added_date', 'birth_year']\n",
    "for col in  im_column_names_to_int:\n",
    "    df_im[col] = pd.to_numeric(df_im[col], errors='coerce', downcast='integer')\n",
    "    \n",
    "df_im[im_column_names_to_int].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert int to date type (part 1)\n",
    "from datetime import datetime\n",
    "def convert_to_datetime(val):\n",
    "    return pd.to_timedelta(val, unit='d', errors='coerce') + datetime(1960, 1, 1)\n",
    "\n",
    "im_column_names_convered_to_date = ['arrival_date', 'departure_date']\n",
    "\n",
    "for col in im_column_names_convered_to_date:\n",
    "    df_im[col] = df_im[col].apply(convert_to_datetime)\n",
    "    \n",
    "df_im[im_column_names_convered_to_date].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert int to date type (part 2)\n",
    "def parse_to_datetime(val):\n",
    "    return pd.to_datetime(val, format='%Y%m%d', errors='coerce')\n",
    "\n",
    "im_column_names_parsed_to_date = ['added_date']\n",
    "\n",
    "for col in im_column_names_parsed_to_date:\n",
    "    df_im[col] = df_im[col].apply(parse_to_datetime)\n",
    "    \n",
    "df_im[im_column_names_parsed_to_date].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_im.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_im.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demograhics dataset\n",
    "\n",
    "This dataset consist of data about the demographics of all US cities and census-designated places with a population greater or equal to 65,000 and comes from the US Census Bureau's 2015 American Community Survey ([source](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/)).\n",
    "\n",
    "This dataset contains following information:\n",
    "- City\n",
    "- State\n",
    "- Median Age\n",
    "- Male Population\n",
    "- Female Population\n",
    "- Total Population\n",
    "- Number of Veterans\n",
    "- Foreign-born\n",
    "- Average Household Size\n",
    "- State Code\n",
    "- Race\n",
    "- Count (race population count in the given city)\n",
    "\n",
    "##### Data source\n",
    "The datasest is saved in single CSV file.<br>\n",
    "The file size is 249kB and it contains about 3,000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the immigration data sample\n",
    "df_demo = pd.read_csv('data/us-cities-demographics.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head data\n",
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_demo.rename(columns= {'City': 'city',\n",
    "        'State': 'state_name',\n",
    "        'Median Age': 'median_age',\n",
    "        'Male Population': 'male_population',\n",
    "        'Female Population': 'female_population',\n",
    "        'Total Population': 'total_population',\n",
    "        'Number of Veterans': 'veterans_count',\n",
    "        'Foreign-born': 'foreigners_count',\n",
    "        'Average Household Size': 'avg_household_size',\n",
    "        'State Code': 'state_code',\n",
    "        'Race': 'race',\n",
    "        'Count': 'race_count'},\n",
    "    inplace=True)\n",
    "\n",
    "df_demo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing data visualization\n",
    "showMissingDataHeatMap(df_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate against 'us states' dictionary\n",
    "count_before = df_demo.shape[0]\n",
    "state_filter = df_demo['state_code'].isin(df_dict_states_codes['code'])\n",
    "df_demo = df_demo[state_filter]         \n",
    "print('Removed rows after us state validation: {}%'.format(((count_before-df_demo.shape[0])/10000)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated values\n",
    "count_before = df_demo.shape[0]\n",
    "print(df_demo[df_demo.duplicated(['city', 'state_name', 'race'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport codes dataset\n",
    "This dataset contains the list of all airport codes from around the world. Some of the columns contain attributes identifying airport locations, other codes (IATA, local if exist) that are relevant to identification of an airport. This dataset was downloaded from public domain source http://ourairports.com/data/ who compiled this data from multiple different sources ([source](https://datahub.io/core/airport-codes#data)).\n",
    "\n",
    "This dataset contains following information:\n",
    "- ident\n",
    "- type\n",
    "- name\n",
    "- elevation_ft\n",
    "- continent\n",
    "- iso_country\n",
    "- iso_region\n",
    "- municipality\n",
    "- gps_code\n",
    "- iata_code\n",
    "- local_code\n",
    "- coordinates\n",
    "\n",
    "##### Data source\n",
    "This dataset is saved in single CSV file.</br>\n",
    "The file size is 5MB and contains about 55,000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the immigration data sample\n",
    "df_air = pd.read_csv('data/airport-codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head data\n",
    "df_air.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air['local_code'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showMissingDataHeatMap(df_air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showMissingDataHeatMap(df_air[df_air['iso_country'] == 'US'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features which have more that 50% of missing values.\n",
    "# From the above we see that following features met above condition: occupation, update_flag, ins_number, visa_issue_department\n",
    "df_air.drop(columns=['iata_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "count_before = df_air.shape[0]\n",
    "df_air = df_air.drop_duplicates(subset=['type', 'name', 'iso_region'], keep='last')\n",
    "print('Removed duplicated for type and name columns: {}%'.format(((count_before-df_air.shape[0])/10000)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature dataset\n",
    "Historical temperature dataset starting from the year 1750 and finishing in the year 2013 ([source](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)).\n",
    "\n",
    "This dataset contains following information:\n",
    "- Date\n",
    "- AverageTemperature\n",
    "- AverageTemperatureUncertainty\n",
    "- City\n",
    "- Country\n",
    "- Latitude\n",
    "- Longitude\n",
    "\n",
    "##### Data source\n",
    "The dataset is saved in single CSV file.<br/>\n",
    "The file size is 500MB and it contains about 8,000,000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the immigration data sample\n",
    "df_temp = pd.read_csv('data/temperature_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head data\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.rename(columns= \n",
    "          {'dt': 'date',  \n",
    "           'AverageTemperature': 'avg_temp',\n",
    "           'AverageTemperatureUncertainty': 'avg_temp_uncertanity',\n",
    "           'City': 'city',\n",
    "           'Country': 'country',\n",
    "           'Latitude': 'latitude',\n",
    "           'Longitude': 'longitude'\n",
    "          },  \n",
    "         inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showMissingDataHeatMap(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values for AverageTemperature\n",
    "count_before = df_temp.shape[0]\n",
    "df_temp = df_temp.dropna(subset=['avg_temp'])\n",
    "print('Removed rows with missing values in column AverageTemperature: {}%'.format(((count_before-df_temp.shape[0])/10000)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "The source data consists of the following datasets:\n",
    "- immigration\n",
    "- demographics\n",
    "- airports\n",
    "- temperature\n",
    "\n",
    "![Datasets](https://app.lucidchart.com/publicSegments/view/b22781a6-a7e7-4a0e-8a24-0c1fbabe12c7/image.png)\n",
    "\n",
    "additional data are extracted from the udacity provided [SAS description file](work/data/I94_SAS_Labels_Descriptions.SAS):\n",
    "- country codes\n",
    "- us ports\n",
    "- us states\n",
    "- visa modes\n",
    "\n",
    "and external sources:\n",
    "- visa types\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "In the project there was adopted star schema with immigration dataset used for the fact table.\n",
    "To simplify the analysis the columns in the fact table (dim_immigration) can be grouped into following classes:\n",
    "- report (year, month)\n",
    "- person (country, gender, age, admission number, added date)\n",
    "- arrival (port, state, date, type, visa, airline, flight)\n",
    "- departure (date)\n",
    "\n",
    "For the fact table there were created following dimension tables:\n",
    "- state (dim_us_state)\n",
    "- port (dim_us_airport)\n",
    "- date (dim_date)\n",
    "- country (dim_country)\n",
    "- arrival type (dim_arrival_type)\n",
    "- visa mode (dim_visa_mode)\n",
    "- visa type (dim_visa_type)\n",
    "\n",
    "##### Dimension tables\n",
    "\n",
    "**dim_us_state** - created by pivoting the demographics dataset by race column and aggregating the data, additionally there were added percentage columns to simplify analitical queries.<br/>\n",
    "**dim_us_ports** - created from subset of the airport dataset as a result of extracting only airports which belong to US.<br/>\n",
    "**dim_date** - created from all the date columns from immigration dataset (added_date, arrival_date, allowed_stay_date, departure_date)<br/>\n",
    "**dim_country** - created by merging the data from 'country codes' dictionary with temperature data set. For the temperature dataset there was used much smaller dataset already aggregated by the country (for the analysis there was used GlobalLandTemperaturesByCity.csv dataset (500MB) for the implementation there was used its much smaller (20MB) already aggregated by country version GlobalLandTemperaturesByCountry.csv)<br/>\n",
    "\n",
    "![Conceptual model](https://app.lucidchart.com/publicSegments/view/51c3d86d-2745-4d07-97e9-08a4ed962a88/image.png)\n",
    "\n",
    "#### 3.1 Physical Data Model\n",
    "\n",
    "Physical data model was created for the Amazon Redshift. \n",
    "There were defined following constraints:\n",
    "- primary keys\n",
    "- foreign keys\n",
    "- unique columns\n",
    "- optional/required columns\n",
    "\n",
    "![Immigration data model](https://app.lucidchart.com/publicSegments/view/f7f609fa-5cb5-4582-8ef8-8c8708a7e424/image.png \"Immigration data model\")\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The steps necessary to pipeline the data into the chosen physical data model are divided into the three categories:\n",
    "- process (raw data is cleaned/transformed and saved into intermediate storage)\n",
    "- load (cleaned data is loaded to the destination warehouse)\n",
    "- check (the quality checks are runned on final data)\n",
    "\n",
    "Since, the dimensions for visa modes, visa types and arrival types are already cleaned and stored in S3 bucket, they are directly loaded into the the warehouse. All other data require prior processing step (cleaned/transformed).\n",
    "\n",
    "![ETL Process](https://app.lucidchart.com/publicSegments/view/429f080b-5aac-4b2e-acbd-58edf622c70b/image.png \"ETL Process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "The data pipeline was implemented in the Apache airflow with the following project structure:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "airflow/\n",
    "├── dags/\n",
    "├── plugins/\n",
    "│   ├── helpers/\n",
    "│   └── operators/\n",
    "└── tests/\n",
    "│   ├── i94project\n",
    "│   ├── tools\n",
    "│   └── test_*.py\n",
    "└── create_img_db.sql\n",
    "└── README.MD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dags folder contain the ETL process which is run on the monthly basis.\n",
    "<br/>The process_* tasks are executing the SparkSubmitOperator.\n",
    "<br/>The load_* tasks are executing custom StageToRedshiftOperator which is defined in plugins/operators folder.\n",
    "<br/>The create_img_db.sql is the final star schema SQL definition file which is run on the Redshfit cluster prior to ETL to create required tables.\n",
    "<br/>The /tests folder contains the unit tests.\n",
    "<br/>The /tests/i94project subfolder contains the local input data for the ETL process unit testing (see folder structure below).\n",
    "\n",
    "<br/>The ETL process operates on the following input data structure :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Amazon S3/\n",
    "└──i94project\n",
    "    ├── stage\n",
    "    │   └── input\n",
    "    │   └── output\n",
    "    └── dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stage/input contains all the data that are processed by process_* tasks:\n",
    " * i94 immigration data (file per month)\n",
    " * airport codes\n",
    " * global temperatures\n",
    " * us cities demographics\n",
    "\n",
    "The dicts folder contains all the required dictionaries:\n",
    " * arrival type\n",
    " * country code\n",
    " * us port\n",
    " * us state\n",
    " * visa mode\n",
    " * visa type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure high quality ETL process there were created unit tests for each of the ETL stages:\n",
    "* cleaning\n",
    "* transform\n",
    "* spark execution\n",
    "\n",
    "Additionally, for the unit tests there was created local folder structure to mimic the S3 buckets ETL source data.\n",
    "\n",
    "The quality checks were implemented by custom DataQualityOperator operator.\n",
    "This operator executes the quality checks which are defined in plugins/helpers/data_quality_checks.json configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "All the dictionaries are included in the folder /airflow/tests/i94project/dicts.\n",
    "<br/>The final data model description can be found in the conceptual and physcial data model secion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Enviroment setup.\n",
    "The simplify the development the Apache Airflow and Apache Spark were run from the same docker image.\n",
    "<br/>The development enviroment was created by: \n",
    "* downloading the docker airflow docker image: https://hub.docker.com/r/puckel/docker-airflow\n",
    "* installing the Apache Spark 3.0.1\n",
    "\n",
    "The final airflow/spark docker image was run with mapped local dag and plugin folder:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker run -d -p 8080:8080 -e FERNET_KEY=\"SOME_GENERATED_KEY\"  -v d:/airflow/dags/:/usr/local/airflow/dags -v d:/airflow/plugins/:/usr/local/airflow/plugins  puckel/docker-airflow:spark webserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "The Amazon S3 bucket is reliable source of the input data.\n",
    "The Apache Airflow allows to automate the ETL process and run it on the monthly basis. Moreover, it creates enviroment which is:\n",
    " * dynamic\n",
    " * built from reusable tasks\n",
    " * easily monitored\n",
    " * easily backfilled\n",
    "\n",
    "The Apache Spark allows efficient big data processing and simple scalling.\n",
    "The Amazon Redshift is opinionated data warehouse and efficient business query processing engine.\n",
    "##### Propose how often the data should be updated and why.\n",
    "The data are generated on the monthly basis, thus the DAG configuration is set up accordingly.\n",
    "##### Write a description of how you would approach the problem differently under the following scenarios: The data was increased by 100x.\n",
    "The whole process should be moved to Amazon EMR cluster with set auto scalling option.\n",
    "##### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "There should be created new DAG run on the daily basis.\n",
    "##### The database needed to be accessed by 100+ people.\n",
    "The redshift cluster should be used with auto scalling option."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
